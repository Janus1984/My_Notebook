### 微调
通过修改预训练网络模型结构（如修改样本类别输出个数），选择性载入预训练网络模型权重（通常是载入除最后全连接层的之前所有层），再用自己的数据集重新训练模型。

#### 不同场景不同做法

- 新数据集比较小、和原数据集相似度高：通常只修改最后几层或者分类层

- 新数据集比较小、和原数据集相似度低：冻结预训练模型的初始k层，对后面n-k层重新训练

- 新数据集比较大、和原数据集相似度高：重新训练所有层，预训练模型只提供权重初始值

- 新数据集比较大、和原数据集相似度低：从头训练所有层（training from scratch）

#### 基本步骤

去掉预训练模型的分类层，接上自己的分类层，以较小的学习率（一般为平时的0.1）来训练网络。如果数据集较小，则冻结前几层权重。

**通常来说**，使用大型数据集做训练后的模型，具备了提取浅层特征和深层特征的能力。所以对于相似的数据分布，可以只对最后几层做微调；对于数据分布较大的数据，可以只保留模型的浅层特征提取能力；而如果自己的数据集也较大的话，可以只利用预训练模型提供权重初始值。一般来说，除非数据集相差太大，fine-tune效果都会比train from scratch好。

- [x] 疑问：微调的思想是把卷积神经网络分为特征提取层+分类层，对于分类层是全连接层的网络可以理解，卷积核提取特征，全连接层分类；但是对于全局池化层的网络，一整个网络都参与了特征提取和分类的任务，微调的方法是否还这么有效？答：有效，但确实有全连接层的网络迁移能力强。参见全连接层总结。



### Training from scratch

#### 初始化

pass

#### Rethinking ImageNet Pre-training

pass

#### ScratchDet

pass