### 微调
通过修改预训练网络模型结构（如修改样本类别输出个数），选择性载入预训练网络模型权重（通常是载入除最后全连接层的之前所有层），再用自己的数据集重新训练模型。

#### 不同场景不同做法

- 新数据集比较小、和原数据集相似度高：通常只修改最后几层或者分类层

- 新数据集比较小、和原数据集相似度低：冻结预训练模型的初始k层，对后面n-k层重新训练

- 新数据集比较大、和原数据集相似度高：重新训练所有层，预训练模型只提供权重初始值

- 新数据集比较大、和原数据集相似度低：从头训练所有层（training from scratch）

#### 基本步骤

去掉预训练模型的分类层，接上自己的分类层，以较小的学习率（一般为平时的0.1）来训练网络。如果数据集较小，则冻结前几层权重。

**通常来说**，使用大型数据集做训练后的模型，具备了提取浅层特征和深层特征的能力。所以对于相似的数据分布，可以只对最后几层做微调；对于数据分布较大的数据，可以只保留模型的浅层特征提取能力；而如果自己的数据集也较大的话，可以只利用预训练模型提供权重初始值。一般来说，除非数据集相差太大，fine-tune效果都会比train from scratch好。

- [x] 疑问：微调的思想是把卷积神经网络分为特征提取层+分类层，对于分类层是全连接层的网络可以理解，卷积核提取特征，全连接层分类；但是对于全局池化层的网络，一整个网络都参与了特征提取和分类的任务，微调的方法是否还这么有效？答：有效，但确实有全连接层的网络迁移能力强。参见全连接层总结。



### Training from scratch

----

**总结**下我个人关于从0训练检测器的3个观点：

1. 需要能够稳定梯度的优化手段（比如clip_gradient、BN、GN、SN、等等）。
2. 训练足够多的epoch与合适的学习率。
3. 对于小数据集，在训练时需要一定的数据增广。

具体可以参看 参考1

#### 初始化

----

先上**结论**：对于tanh激活函数，使用xavier初始化；对于relu激活函数，使用He初始化。

##### 权值初始化的目的

权值初始化的目的是

1. 非凸优化问题中一个好的初始点，能更有利于收敛到理想的结果。
2. 几个较大的权值相乘（链式法则）会在loss地形上形成斜率（$ ∂J/∂ω$）较大的“悬崖结构”。导致梯度爆炸，使得更新的参数突变，从而使得前面已完成的大量优化工作无效。在RNN中尤为常见。反之，较小的权值相乘（再加上sigmoid的导数<=1/4），会造成梯度消失。(花书p.177)
3. glorot条件：好的初始化应该使得每层的激活值和梯度的方差在传播过程中保持一致。前者有利于平稳的学习，后者有利于信息的反馈。

##### 初始化的方法

- [x] todo 参考3

- Xavier
- He

#### Rethinking ImageNet Pre-training

pass

#### ScratchDet

pass



#### 参考

1. [如何评价何恺明等 arxiv 新作 Rethinking ImageNet Pre-training？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/303234604)

2. [神经网络中的权值初始化：从最基本的方法到Xavier、he初始化方法一路走来的历程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/86602524)

3. [【AI初识境】什么是深度学习成功的开始？参数初始化（xavier，he等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/57454669)

